Kafka高可用

# 1. 简介

在没有Replication的情况下，一旦某机器宕机或者某个Broker停止工作则会造成整个系统的可用性降低。随着集群规模的增加，整个集群中出现该类异常的几率大大增加，因此对于生产系统而言Replication机制的引入非常重要。 发往故障broker的数据将会丢失。

**在引入了replication后，同一个partition会存在多个replica，那么需要在replica中选出一个leader，producer和consumer都和这个leader交互，其他的replica都作为follower从leader复制数据。**

需要保证同一个partition的多个replica数据的一致性所以leader负责数据的读写，follower只向leader顺序fetch数据，系统更加简单和高效。

# 2. 高可用设计

更好的负载均衡，需要将一个topic的多个partition均分到不同的broker上。所以分配的思路如下

1.  将第i个Partition分配到第（i mod n）个Broker上
2.  将第i个Partition的第j个Replica分配到第（(i + j) mod n）个Broker上

![](https://medesqure.oss-cn-hangzhou.aliyuncs.com/img/20200715211307.png)



## 2.1 producer如何投递消息到broker

producer在发布消息到topic时，先根据route规则路由到某个partition。**producer会根据broker获取集群的信息并缓存在本地。**再找到partition的leader发送数据。Leader会将消息写入本地Log(顺序写文件)。Follower会从Leader批量pull数据(会批量拉取)。**当leader收到ISR中所有replica的ACK，该消息就会被认为是commit。Leader将增加HW并且向producer发送ACK。**

- 为了提高性能，每个Follower在收到数据后会向Leader发送Ack，再将数据写入到Log中。所以对于commit的消息，kafka只能保证数据被存在多个replica的内存中而不保证数据被持久化到磁盘。
- consumer在读取消息也是从leader上读取，只有被commit的消息(offset小于HW的消息)才会暴露给consumer。

## 2.2 ACK前需要保证多少个备份

kafka采用的是ISR(in-sync replica)的机制来保证数据的同步。如果一个follower所在的broker宕机或者落后太多，Leader会将其从ISR中移除。移除的条件

- follower复制的消息落后leader的条数超过4000(默认值)
- follower超过一定的事件没有向leader发送fetch请求(默认值)

**数据的复制有同步复制和异步复制**

- 同步复制
  所有的follower都复制完消息，该消息就被认为commit，这种复制方式影响吞吐率。
- 异步复制
  数据被写入leader的文件就算成功，follower都是异步向leader同步数据。这种情况下follower都是落后leader，如果Leader宕机，就会丢失数据。

**kafka的ISR机制平衡了这两种优缺点，数据不丢失且吞吐率搞。Follower可以批量从Leader拉取数据提高复制的性能。**

一条消息被ISR中的所有Follower都复制了才会被认为已提交(**延迟在一定的容忍范围内**)。这样就可以避免宕机造成的数据丢失。对于producer可以选择等待leader的commit。

## 2.3 Leader 的Election算法

当Leader宕机，如何从Follower选择出新的Leader。这里有类似于raft的选举

- Majority Vote  如果有2N+1个节点，在commit时要保证有N+1个replica复制完数据，否则在选举会存在数据丢失的情况。而且在这么多的节点中，最多只能允许N个节点失败。**这个算法的缺点是为了保证较高的容错程度，要有大量的replica，但是大量的replica会导致性能下降**。

kafka采用的是在zookeeper上维护了一个ISR。这个ISR里所有的replica都跟上了Leader。只有ISR的follower才能选举成为Leader。

好处:

 一个partition在保证不丢失消息的情况下可以容忍f个replica的失败。

### 2.3.1 如何处理Replica均不工作

当ISR中存在的follower都不工作，无法保证commit的数据不会丢失。但是partition的所有replica都宕机了，就无法

1. 等待ISR中第一个Replica活过来并选为Leader（保证一致性）
2. 选择第一个Replica活过来Leader （保证高可用）



## 2.4 选举Leader

最简单的方案是每个Follower都在zk上设置一个Watch，当Leader挂了，所有的Follower都尝试去创建该节点。而zk保证了只有一个节点能创建成功。**Leader在zk内创建了一个节点，其他的follower会监听这个节点**。

但是该算法有如下的问题

1. 当leader挂了，zk不能在同一时间都通知到所有的follower，每个follower响应不一致。
2. 当broker内的partition较多，就会造成多个watch被触发，造成集群内大量的调整
3. zk的负载过重，当集群增加到几千个partition时，那么replica都要注册一个watch。

**kafka采用的是用controller来控制集群的状态。在所有的broker选出一个controller。所有的partition的Leader选举都由controller控制。同时也负责topic的增加和删除以及replica的重新分配。controller通过RPC的方式和其他的broker通信。**

![](https://medesqure.oss-cn-hangzhou.aliyuncs.com/img/20200716165509.png)

### 2.4.1 broker failover选举过程

1. controller在zk上注册watch监听，一旦有broker挂了，zk会删除该znode并且发送通知给controller。controller再去zk拉取最新的幸存的broker列表。
2. controller读取挂了的broker的partition列表set_p 在zk的路径为`/brokers/topics/partitions/stat`
3. 对于set-p的每一个partition，读取该partition的ISR，并从ISR中选出新的Leader。如果partition中的所有Replica都挂了，那么将新的leader设为-1
4. 将新的leader、ISR写回到zk。

以上的操作都是controller根据set_p向相应的broker发送RPC命令来完成的。

## 2.5 新增删除Topic操作

controller会在zk的`brokers/topics`节点注册Watch，一旦topic被创建或者删除，controller就会得到通知。

- 删除topic 
  删除了topic后，zk会在`/admin/delete_topics`目录下创建该topic名称。controller被通知后就会去各个broker上删除partition

- 创建topic

  controller得到创建topic的通知后，会从`/brokers/ids`中读取当前的可用brokers列表，在创建这个topic的partition列表并选择leader和对应的follower。**先将这些数据写回zk，然后再发送通知到对应的broker**

## 2.6 Broker响应controller

broker采用的是NIO的模式来通信。采用reactor模型，一个Acceptor来接收客户单请求，N个Processor负责读写数据，M个Handler处理业务

Acceptor负责监听Producer、Consumer、Controller、Admin Tool的链接请求。

Broker在启动时会根据其ID在zk的`/brokers/ids`的znode上创建零时节点。

## 2.7 Controller failover

controller挂了也需要故障转移。每个broker都会在controller path(/controller)上注册一个watch。当controller失败就会发送event。其他的broker会竞选成为controller。**成功就会创建节点，失败就去watch。**

在broker竞选成为新的controller后，会去zk上的各个节点去监听事件。监听topic、broker、等事件

## 2.8 Follower从Leader 批量拉取数据

![](https://medesqure.oss-cn-hangzhou.aliyuncs.com/img/20200716175247.png)

这个函数是follower向Leader拉取数据还有consumer向broker拉取数据通用的方法。

重要的参数有 **最大等待时间和 最小获取字节数**

Leader收到Follower的fetch请求后，响应过程如下

1. 先读取数据并放在dataRead中
2. 请求来自Follower，就更新响应的offset一级对应partition的HW。
3. 根据dataread取出可读消息长度并存在缓存中。（要不要发送给客户端 看客户端是否要等待，数据量是否足够）发送给客户端
4. 将数据返回给客户端

## 2.9 partition的负载均衡

负载均衡包括2个方面。1、broker宕机后导致重新选举导致的leader不均衡。2、加机器后导致的partition的重新分配。

原理

1. 在zk上创建`/admin/preferred_replica_election`节点，并存入需要调整的partition信息。
2. controller会watch该节点，一旦节点被创建，就会收到通知并获取内容。
3. controller会读取需要更换的replica，如果该replica在原ISR列表中，就让其成为leader。否则为了避免数据丢失并不会选择为Leader。









