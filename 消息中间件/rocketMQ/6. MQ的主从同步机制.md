MQ的主从同步

# 1. 概要

为了提高消息消费的高可用性，避免 Broker 发生单点故障引起存储在 Broker 上的消息无法及时消费，RocketMQ 引入了 Broker 主从机制：即消息 消费到达Master后需要消息同步到消息从服务器Slave，如果Master宕机后，消息消费者可以从Slave拉取消息。

同时 RocketMQ 依赖 NameServer, 所以为了确保高可用，同时要确保 NameServer 的高可用，一般通过部署多台 NamesServer 服务器来实现，但彼此 之间互不通信，也就是 NameServer 务器之间在某一时刻的数据并不会完全相同，但这对消息发送不会造成任何影响，这也是 RocketMQ NameServer 设计的一个亮点。

![img](https://medesqure.oss-cn-hangzhou.aliyuncs.com/img/20200803221022.png?ynotemdtimestamp=1596470718949)

# 1.1 MQ的集群部署方式

- 多Master-多Slave 异步复制

优点： 即使磁盘损坏，消息丢失的非常少，且消息实时性不会受影响，同时Master宕机后，消费者仍然可以从Slave消费，而且此过程对应用透明，不需要人工干预，一般情况下都是 master 消费，在 master 宕机或超过负载时，消费者可以从 slave 读取消息，消息的实时性不会受影响，性能几乎和多 master 一样。

缺点：使用异步复制的同步方式有可能会存在消息丢失的问题。

- 多Master-多Slave 同步双写

优点：同步双写保证数据不会丢失。数据和服务都不会单点故障。在M挂了，消息不会丢失。

缺点: 性能比异步复制略低(10%)，且producer发送消息的rt较高。目前的版本M挂了后S不会自动切换成为新的M。

> 同步刷盘和异步刷盘指的是commitLog的文件同步到磁盘是同步还是异步。

对数据要求较高的场景，**建议的持久化策略是主 broker 和从 broker 采用同步复制方式，主从 broker 都采用异步刷盘方式**。通过同步复制方式，保存数据热备份，通过异步刷盘方式，保证 rocketMQ 高吞吐量。

## 1.2 MQ的主从同步机制

主从同步的机制如下

1. Master启动并在指定端口监听
2. Slave启动，主动连接Master，建立TCP连接。
3. Slave 每5s的时间间隔从服务端拉取消息，如果是第一次拉取，就从本地的commitLog文件中的最大偏移量，以该偏移量向服务端拉取消息。
4. 服务端解析请求，并返回一批数据给客户端。客户端收到消息，将消息写入本地的commitLog文件中，然后向Master汇报拉取进度，并更新下一次的待拉取offset。

![img](https://medesqure.oss-cn-hangzhou.aliyuncs.com/img/20200803213047.png?ynotemdtimestamp=1596470718949)

MQ主从同步的一个特征是 ： 主从同步不具备主从切换功能。当主节点挂了，从节点不具备消息写入的功能，但是可以提供消息的读取功能。

# 2. MQ的主从读写分离机制

 MQ的主从机制，在默认情况下Consumer还是优先从M中拉取消息。

MQ判断从M中拉取还是从S中拉取的判断代码如下

```java
long diff = maxOffsetPy - maxPhyOffsetPulling;
long memory = (long) (StoreUtil.TOTAL_PHYSICAL_MEMORY_SIZE
                            * (this.messageStoreConfig.getAccessMessageInMemoryMaxRatio() / 100.0));
getResult.setSuggestPullingFromSlave(diff > memory);   
```

其中 maxOffsetPy是当前的最大偏移量，maxPhyOffsetPulling是消费者拉取的偏移量。

这个diff值通常用来理解当前未处理的消息总大小。memory是MQ存储在PageCache中的内库大小。

**触发下次从S拉取的条件为：当前所有可用消息数据(所有commitlog)文件的大小已经超过了其阔值，默认为物理内存的40%。**

读写分类的机制引入的目的是当消息堆积的内容超过了物理内存的40%，则消息从S中拉取，避免M的缺页导致的IO抖动。

# 3. MQ的消费进度管理

在集群模式下，消息消费的进度存储在broker当中，持久化在store/config/comsumerOfset.json当中。Consumer从服务器拉取一批消息并提交到ConsumeMessageService消费消息后，当消息消费成功会定期向broker发送ACK，broker收到consumer的拉取offset会持久化到磁盘中。

Consumer在上报消息拉取的offset会默认选择M。因为M的brokerId为0，默认当M存活，就优先选择M，只有M宕机，才会选择S。

## 3.1 OffsetStore更新进度到broker

集群消费Consumer采用`RemoteBrokerOffsetStore#updateConsumeOffsetToBroker`方法定期将消费进度上报到Broker。其中的关键逻辑是选择broker。

**如果M存活，则选择M，如果M宕机，则选择S。也就是说，不管消息是M拉取的还是从S拉取的，提交消息消费进度请求，优先选择M。服务端就是接收其偏移量，更新到服务端的内存中，然后定时持久化到${ROCKETMQ_HOME}/store/config/consumerOffset.json。**

## 3.2 M-S之间的消费进度同步

当消费进度被提交到M当中，如果此时M挂了，S会接管消息的拉取工作。此时消息的拉取offset是存储在S当中。那么M和S之间的消费进度会出现不一致的情况，并且当M恢复后，两者的消费进度如何同步？？？

**如果broker是S-broker，那么会通过定时任务调用syncAll(), 从M中定时同步topic路由信息，消息消费的进度，延迟队列处理进度，消费组的订阅信息。**

如果M重启后，S又会从M中同步消息消费进度，会发生重复拉取消息的问题？MQ提供了2种机制确保不会丢失消息的消费进度

1. consumer在内存中保存了最新的消费进度(offsetStore)。消息拉取会以该进度去broker当中拉取。并且会定时向M-Broker反馈消息消费进度。此时broker会立即更新消费进度并且持久化。那么S-Broker也会定时同步最新的消费进度。
2. consumer向M-broker拉取消息时如果上报了当前的消费进度，也会触发消息进度的更新。

# 4. 总结

1. M-S服务器都在运行过程中，消息消费者是从M拉取消息还是从S拉取？
   默认情况下，RocketMQ消息消费者从M-broker拉取，当M-broker积压的消息超过了物理内存的40%，则建议从S-broker拉取。但如果slaveReadEnable为false，表示从服务器不可读，从服务器也不会接管消息拉取。

2. 消息消费者向S-broker拉取消息后，会一直从S-broker拉取？
   分如下情况：
   - 如果S-broker的slaveReadEnable设置为false，则下次拉取，从M-broker拉取。
   - 如果S-broker允许读取并且S-broker堆积的消息没有到内存40%，就从M中-broker拉取(**因为M中也可能没有到达**)。如果S-broker中堆积的消息到达内存40%，则从S-broker中拉取。

3. 主从服务消息消费进是如何同步的？
   消息消费进度的同步时单向的，S开启一个定时任务，定时从M同步消息消费进度；无论消息消费者是S拉的消息还是从M拉取的消息，在向Broker反馈消息消费进度时，优先向M汇报；消息消费者向M拉取消息时，如果消费者在内存中存在消息消费进度时，M会尝试跟新消息消费进度。
4. 读写分离的正确使用姿势：
   1、主从Broker服务器的slaveReadEnable设置为true。
   2、通过updateSubGroup命令更新消息组whichBrokerWhenConsumeSlowly、brokerId，特别是其brokerId不要设置为0，不然从从服务器拉取一次后，下一次拉取就会从主去拉取。