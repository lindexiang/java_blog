[TOC]
# 1.简介

业务系统需要MQ有持久化存储的能力，能大大增加系统的可用性。在存储效率和方式来看，从效率看文件系统高于KV系统，高于关系型数据库。从可靠性看是相反的。

保证高可靠，那么数据就必须持久化到磁盘上，将数据持久化到磁盘，那么可能就不能保证高性能了。

## 1.1 磁盘的顺序写和随机写

- 顺序写

  SSD固态硬盘在顺序写的速度为1500M/s，普通磁盘顺序写的速度可以达到 450MB/s~600MB/s。

- 随机写

  磁盘在随机写速度急速下降，磁盘的随机写速度可能只有几百KB/s，这远远要慢于网络传输速度，所以它并不能满足高性能的要求，要避免随机写入。**因为磁盘的寻找轨道是最最最慢的操作。**

## 1.2 MQ的存储简介

RocketMQ 在持久化的设计上，采取的是**「消息顺序写、随机读的策略」**，利用磁盘顺序写的速度，让磁盘的写速度不会成为系统的瓶颈。并且采用 MMPP 这种“零拷贝”技术，提高消息存盘和网络发送的速度。极力满足 RocketMQ 的高性能、高可靠要求。

存储的整体结构图如下所示

![](https://medesqure.oss-cn-hangzhou.aliyuncs.com/img/20200805165800.png)

- **CommitLog**   消息的真正存储文件，所有topic的消息都存储在该文件中。**每台Broker的commitLog文件被本机所有的Topic所共享。**
- **ConsumeQueue**    消息的逻辑队列，类似kafka中的partition，存储了消息在CommitLog中的索引。
- **IndexFile**   消息索引文件，存储Key和offset的对应关系，提升消息的检索速度。

## 1.3 ConsumeQueue的作用

CommitLog 文件是存放消息数据的地方，所有的消息都将存入到 CommitLog 文件中。生产者将消息发送到 RocketMQ 的 Broker 后，Broker 服务器会将**「消息顺序写入到 CommitLog 文件中」**，这也就是 RocketMQ 高性能的原因，因为我们知道磁盘顺序写特别快，RocketMQ 充分利用了这一点，极大的提高消息写入效率。

但是消费者消费消息的时候，可能就会遇到麻烦，每一个消费者只能订阅一个主题，消费者关心的是订阅主题下的所有消息，但是同一主题的消息在 CommitLog 文件中可能是不连续的，那么**「消费者消费消息的时候，需要将 CommitLog 文件加载到内存中遍历查找订阅主题下的消息，频繁的 IO 操作，性能就会急速下降」**。

RocketMQ 引入了 Consumequeue 文件。**「Consumequeue 文件可以看作是索引文件，类似于 MySQL 中的二级索引」**。在存放了同一主题下的所有消息，消费者消费的时候只需要去对应的 Consumequeue 组中取消息即可。Consumequeue 文件不会存储消息的全量信息，了解 MySQL 索引的话，应该好理解这里，具体存储的字段，我在上图已经标注。这样做可以带来以下两个好处：

- 由于 Consumequeue 文件内容小，可以尽可能的保证 Consumequeue 文件全部读入到内存，提高消费效率。
- Consumequeue 文件也是会持久化的，不存全量信息可以节约磁盘空间。

## 1.4 MQ和Kafka的存储方式的区别

kafka的每个partition都是一个独立的物理文件，消息直接从里面读写。

![img](https://medesqure.oss-cn-hangzhou.aliyuncs.com/img/20200721132935.png?ynotemdtimestamp=1595436859525)

一旦kafka中Topic的partitoin数量过多，队列文件会过多，会给磁盘的IO读写造成很大的压力，造成tps迅速下降。**顺序写磁盘会变成随机写磁盘**

> kafka的吞吐量高，每个topic都有自己的partition文件。但是对topic的并发度不高。当topic过大时，kafka的性能会急剧下降。而RocketMq的吞吐量没有kafka高，因为加锁写单个文件，并且无法利用多个磁盘的并发写，但是可以支持的topic很高。

- **MQ的优点** 
  对磁盘的访问串行化，避免topic并发写入导致的磁盘竟争，不会因为队列增加导致IOWAIT增高。
- MQ的缺点
  1. 写虽然完全是顺序写，但是读却变成了完全的随机读。有PageCache和预读策略，也不会出现大量的IO读。
  2. 读一条消息，会先读ConsumeQueue，再读CommitLog，增加了开销。
  3. 要保证CommitLog与ConsumeQueue完全的一致，增加了编程的复杂度。

## 1.5 MQ提升随机读的优化思路

1. 增大内存，让pageCache尽可能的缓存更多的数据，减少IO操作。
2. 数据预加载策略 访问时，即使只访问1k的消息，系统也会提前预读出更多数据，在下次读时，就可能命中内存。
3. 系统的IO调度算法优化 OS在IO调度的算法优化上已经做了很多。
   **在读取多个pageCache时，IO的算法会将其进行排序和合并。将完全随机改成顺序跳跃的方式。*可以大大减少磁盘寻址这个最最最慢的操作*。**顺序跳跃方式读较完全的随机读性能会高5倍以上。另外4k的消息在完全随机访问情况下，仍然可以达到8K次每秒以上的读性能。

因此，ConsumeQueue存储数据量极少而且是顺序读，在PAGECACHE预读作用下，ConsumeQueue的读性能几乎与内存一致，即使堆积情况下。所以可认为ConsumeQueue完全不会阻碍读性能(相比于kafka的机制还是会慢一些)。**CommitLog中存储了所有的元信息，包含消息体，所以只要有CommitLog在，ConsumeQueue即使数据丢失，仍然可以恢复出来。**

# 2.  MQ的消息存储文件

## 2.1 CommitLog 文件

CommitLog的文件存储逻辑如下，所有的Topic是混合存储在一个队列里。每一条消息的前面4个字节存储了该消息的总长度。

![img](https://medesqure.oss-cn-hangzhou.aliyuncs.com/img/20200721210303.png?ynotemdtimestamp=1595436859525)

​	在查找消息时先根据offset查找消息的起始位置，根据消息的长度。再去捞出单条消息。

## 2.2 ConsumeQueue文件

消费者不会直接去消费commitLog文件中查找订阅主题下的消息。而是采用ConsumeQueue队列索引。

broker会为每个Topic都创建4个QueueId。每个QueueId都是一个文件夹，分段存储ConsumeQueue。可以可以看成是kafka的partition。**这里有0-3是说明该topic存在4个ConsumeQueue**。

![img](https://medesqure.oss-cn-hangzhou.aliyuncs.com/img/20200721214456.png?ynotemdtimestamp=1595436859525)

ConsumeQueue只存储的消息的索引，包括消息的起始offset，size和tag的hashcode。**因为每个消息的长度不一样，所以要用offset+size去定位一条消息。**

![img](https://medesqure.oss-cn-hangzhou.aliyuncs.com/img/20200721214208.png?ynotemdtimestamp=1595436859525)

每个ConsumeQueue文件包含30w条记录，可以看成是索引数组。每个文件的长度是30W*20个字节。

**消息消费进度存储的偏移量是逻辑偏移量**。根据index*20获取消息的commitLogOffset+size。再从commitLog中查找消息。

如果小于minLogicOffset，说明文件已经被删除。(**每个文件的标题就是当前的偏移地址**)如果大于minLogicOffset就定位到具体的物理文件，然后再取模得到在文件中的偏移量，定位到该消息的物理偏移量。

## 2.3 Index索引文件

Index索引文件是Hash索引机制为消息建立索引。可以根据消息的Key快速查找出消息所在的位置。类似于HashMap。这个就不详细说了

![img](https://medesqure.oss-cn-hangzhou.aliyuncs.com/img/20200721215403.png?ynotemdtimestamp=1595436859525)

# 3. 消息存储的原理

## 3.1 关键类

- DefaultMessageStore          消息存储的最重要类，包含很多消息文件操作API。
- CommitLog                            CommitLog文件的存储实现类
- IndexService                          索引文件实现类
- ReputMessageService         消息分发，构建ConsumeQueue、IndexFile文件
- MappedFile                           每个MappedFile对应的是物理上的磁盘文件的内存映射。
- MappedFileQueue              维护了MappedFile数组的映射

![img](https://medesqure.oss-cn-hangzhou.aliyuncs.com/img/20200721132958.png?ynotemdtimestamp=1595436859525)

## 3.2 MappedFile内存映射文件

MappedFile是内存映射文件的具体实现。

```java
1. OS_PAGE_SIZE      	// 操作页的大小 默认是4K
2. committedPosition    // 文件提交位置             
和FlushedPosition       // 文件刷盘位置
3. FileChannel          // 文件通道
4. ByteBuffer           // 堆外内存，即DirectBuffter,数据先存在堆内存中。再提交到MappedFile对应的
                        //  内存映射MapedByteBuffer
5. MappedByteBuffer     //  磁盘文件对应的内存映射buffer
6. TransientStorePool   //  堆内存池
```

- TransientStorePoolEnable=true
  启用堆外内存池。消息是先写在堆外的DirectByteBuffer内存，再通过Commit线程将数据提交到MappedByteBuffer，再通过OS的Flush线程持久化到磁盘中。
- TransientStorePoolEnable=false
  数据直接写在MappedByteBuffer,再通过OS的flush线程刷到磁盘中。

### 3.2.1 MapedFIle的commit

当采用先写DirectByteBuffer再写MapedByteBuffer的方式时，需要有MapedFile的commit操作。如果是采用直接写MapedByteBuffer的方式，commit直接返回。

步骤是
**获取磁盘文件对应的FileChannel，将MappedFile的writeBuffer中的数据写到FIleChannel当中，再调用force()操作将数据刷到磁盘中。**

## 3.3 MapedFileQueue映射文件队列

MapedFileQueue是MapedFile的管理容器，对存储目录的封装，例如commitLog的存储路径下会有多个内存映射文件(MappedFile)。

mappedFileQueue的包含的属性

```java
1. List<MappedFile>  映射文件列表
2. flushedWhere      当前刷盘指针，该指针之前的数据全部持久化到磁盘中
3. commitedWhere    数据提交指针，内存中ByteBuffer当前的写指针，该值大于flushedWhere
```

**MappedFileQueue提供了多种维护查找MappedFile。**

- 按照消息存储时间查找MappedFile

  MappedFile有最后一次更新时间戳。遍历所有的MappedFile列表，用时间戳来查找。

- **按照消息偏移量offset查找MappedFile**

  **由于受到磁盘的大小限制，当磁盘的大小不足会删除早期的commitLog文件。**所以offset对应的消息可能已经被删除。所以需要获取该MappedFileQueue的minOffset和maxOffset，所以定位offset

  ```java
  offset = （index-minOffset）/FileSize
  ```

- 获取MappedFileQueue消息的最小偏移量

  因为每个文件的名字就是起始offset，所以最小偏移量就是第0个MappedFile的起始偏移地址

- 获取MappedFileQueue消息的最大偏移量

  最后一个MappedFIle的起始偏移地址+commitedWhere当前写指针地址。

## 3.4  TransientStorePool 的作用

短暂的堆外内存池。作用是用来零时缓存数据。数据先被写入DirectByteBuffer堆外内存中，然后采用commit线程定时将数据从堆内存复制到MappedByteBuffer当中。再由Flush线程将数据写到文件中。**采用该策略是为了将堆外内存一直锁定在内存中，防止被进程将内存交换到磁盘中。**

## 3.5 DefaultMessageStore#putMessage存储消息的流程

1. 如果当前的**broker停止工作或slave模式或磁盘满**，拒绝写入消息。消息不能超过65536个字符。只有Master可以写消息。**如果是延时消息，替换实际的topic为SCHEDULE_TOPIC_XXX**
3. 获取当前可以写入的CommitLog文件。每个文件默认1G，以偏移量作为文件名。**获取CommitLog的writeLock，即写入的操作时串行的，kafka可以利用topic的partition并发。**
5. 从commitLog中获取MapedFileQueue的LastMappedFile，将消息写入MapedFIle中并更新offset。
6. 创建全局唯一的消息ID，组成为IP+port+offset。**所以根据MsgId可以直接查找消息。**
7. 处理完消息后释放Lock。消息此时只是追加到内存中。再根据同步刷盘或者异步刷盘策略持久化到磁盘中。
6. reputMessageService将异步将消息的索引写入到ConsumeQueue和IndexFIle当中。

![](https://medesqure.oss-cn-hangzhou.aliyuncs.com/img/20200724111650.png)

## 3.6 消息数据刷盘机制

操作系统PageCache的存在，PageCache是OS对文件的缓存，用户加速对文件的读写。写磁盘一般是先写到PageCache当中，再持久化到磁盘中。熟悉的Mysql和Redis都是这么操作。**持久化刷盘的操作很简单，调用MapedFileQueue.force()或者FIleChannel.force()方法即可刷盘。**

MQ提供了**同步刷盘和异步刷盘**2种刷盘方式。

- 异步刷盘
  消息写入到内存的 PAGECACHE中，就立刻给客户端返回写操作成功，后台线程一般是5s定时将 PAGECACHE 中的消息写入到磁盘中。这种方式**「吞吐量大，性能高，但是 PAGECACHE 中的数据可能丢失，不能保证数据绝对的安全」**。

- 同步刷盘
  消息写入内存的 PAGECACHE 后，立刻通知刷盘线程刷盘，然后等待刷盘完成，刷盘线程执行完成后唤醒等待的线程，返回消息写成功的状态。这种方式**「可以保证数据绝对安全，但是吞吐量不大」**。

  

![](https://medesqure.oss-cn-hangzhou.aliyuncs.com/img/20200805220125.png)

## 3.7 ReputMessageService更新ConsumeQueue和Index

在一个broker的每个topic可能会存在多个ConsumeQueue队列，采用QueueId来标记。

Broker在启动时会启动ReputMessageService线程，并初始化了**reputFormOffset**，该值标记了CommitLog的更新的位置。该线程每1ms会尝试推送消息到ConsumeQueue和Index文件。更新的步骤如下

1. 返回reputFromOffset开始的全部有效数据(commitlog文件)并循环读取每一条消息。
2. 从ByteBuffer中循环读取消息，根据消息的topic和QueueId获取对应的consumeQueue
3. 将消息的offset，长度，tag写入到ByteBuffer，将内容追加到ConsumeQueue的内存映射文件中。由OS异步刷盘。

# 4. 重要的机制详细说明

## 4.1 消息发送的MMAP零拷贝

`Linux`操作系统分为【用户态】和【内核态】，文件操作、网络操作需要涉及这两种形态的切换，免不了进行数据复制。

一台服务器把本机磁盘文件的内容发送到客户端，一般分为两个步骤：

1）`read`：读取本地文件内容；

2）`write`：将读取的内容通过网络发送出去。

**这两个看似简单的操作，实际进行了`4` 次数据复制，分别是：**

1. **从磁盘复制数据到内核态内存；**
2. **从内核态内存复制到用户态内存；**
3. **然后从用户态内存复制到网络驱动的内核态内存；**
4. **最后是从网络驱动的内核态内存复制到网卡中进行传输。**

通过使用`mmap`的方式，可以省去向用户态的内存复制，提高速度。这种机制在`Java`中是通过`MappedByteBuffer`实现的。

`RocketMQ`充分利用了上述特性，也就是所谓的“零拷贝”技术，提高消息存盘和网络发送的速度。

> 这里需要注意的是，采用`MappedByteBuffer`这种内存映射的方式有几个限制，其中之一是一次只能映射`1.5~2G` 的文件至用户态的虚拟内存，这也是为何`RocketMQ`默认设置单个`CommitLog`日志数据文件为`1G`的原因了还有如果一直缺页置换也会导致性能极差。

## 4.2  JDK的IO和NIO说明

主要区别如下

1. IO是面向流，NIO是面向缓冲区的。java IO每次都要读取所有的字节不能缓存在任何地方。NIO是将数据读取到可以稍后处理的缓冲区中。
2. java IO流是阻塞的，当线程发起read()或者write()操作后就阻塞直到数据被操作完毕。NIO是非阻塞的，从channel读取数据时如果没有数据就立即返回。
3. Selectors java NIO使用selector线程来监视多个通道。这种机制使一个线程可以管理多个channel

NIO中也有2种Buffer。

1. **ByteBuffer** 
   堆内存缓冲区，在数据量少的时候可以使用。
2. **MappedByteBuffer** 
   文件映射，在数据量大时，比如1G的文件，可以将文件直接映射到内存中(虚拟内存)。通常可以直接映射整个文件，如果文件较大，需要进行分段映射。

这是NIO中的DirectBuffer和MappedByteBuffer来提高IO效率

- DirectBuffer
  堆外内存，JVM会使用native IO操作。使用DirectBuffer可以避免额外的数据复制，提高IO效率，同时减少GC的工作量。

- MappedByteBuffer
  也是DirectBuffer，但是是将文件内容映射到内存中。省去了内核空间到用户空间的拷贝。

```java
ByteBuffer buffer =  ByteBuffer.allocateDirect(1024);  
MappedByteBuffer buffer = fileChannel.map(MapMode.READ_ONLY, 0, 1024);  
```



![](https://medesqure.oss-cn-hangzhou.aliyuncs.com/img/20200724010643.png)

## 4.3  消息刷盘中的详细步骤

### 4.3.1 同步刷盘流程

同步刷盘采用的是producer-consumer模型。消息被追加到MappedFile后立即将数据从内存刷到磁盘中。**由CommitLog类的handleDIskFlush实现。** 步骤如下

1. 构建GroupCommitRequest同步任务请求并提交到GroupCommitService。
2. producer调用阻塞等待刷盘结果，超时时间为5s
3. GroupCommitService 线程处理GroupCommitRequest对象后就调用wakeUpCustomer方法唤醒producer。

![](https://medesqure.oss-cn-hangzhou.aliyuncs.com/img/20200723013031.png)

具体的代码如下

- Commitlog#handleDiskFlush   提交刷盘任务

```java
void handleDiskFlush() {
  //.....
  final GroupCommitService service = (GroupCommitService) this.flushCommitLogService;
  GroupCommitRequest request= new GroupCommitRequest(result.getWroteOffset() +
                                                     result.getW 	roteBytes());
  service.putRequest(request); //提交刷盘request
  boolean flushOK = request.waitForFlush(  //开始同步等待返回结果
    this.defaultMessageStore.getMessageStoreConfig().getSyncFlushTimeout();	
    if( !flushOK){
      putMessageResult.setPutMessageStatus(PutMessageStatus.FLUSH DISK TIMEOUT);
    }
}
```

- GroupCommitRequest   

```java
//GroupCommitRequest
public static class GroupCommitRequest {
  //这次请求要刷到的offSet位置，比如已经刷到2，
  private final long nextOffset;
  //控制flush的拴
  private final CountDownLatch countDownLatch = new CountDownLatch(1);
  private volatile boolean flushOK = false;

  //刷完了唤醒
  public void wakeupCustomer(final boolean flushOK) {
    this.flushOK = flushOK;
    this.countDownLatch.countDown();
  }
  
  public boolean waitForFlush(long timeout) {
    try {
      this.countDownLatch.await(timeout, TimeUnit.MILLISECONDS);
      return this.flushOK;
    } catch (InterruptedException e) {
      e.printStackTrace();
      return false;
    }
  }
}
```

- GroupCommitService   启动线程做具体的刷盘任务

为了避免同步刷盘任务和其他producer提交的任务直接的锁竞争，提供了读List和写List。每次将读List先拷贝到写List中再去消费。

GroupCommitService每处理一批同步刷盘请求后就会休眠10ms

执行刷盘操作就是调用MappedFileQueue的flush的flush操作，即MappedByteBuffer的flush操作。

处理完刷盘任务后腰更新checkpoint刷盘点。

```java
class GroupCommitService extends FlushCommitLogService {
    //用来接收消息的队列，提供写消息
    private volatile List<GroupCommitRequest> requestsWrite = new ArrayList<GroupCommitRequest>();
    //用来读消息的队列，将消息从内存读到硬盘
    private volatile List<GroupCommitRequest> requestsRead = new ArrayList<GroupCommitRequest>();

    //添加一个刷盘的request
    public void putRequest(final GroupCommitRequest request) {
        synchronized (this) {
            //添加到写消息的list中
            this.requestsWrite.add(request);
            //唤醒其他线程
            if (!this.hasNotified) {
                this.hasNotified = true;
                this.notify();
            }
        }
    }

    //交换读写队列，避免上锁
    private void swapRequests() {
        List<GroupCommitRequest> tmp = this.requestsWrite;
        this.requestsWrite = this.requestsRead;
        this.requestsRead = tmp;
    }

	//具体的刷盘doCommit
    private void doCommit() {
      for (GroupCommitRequest req : this.requestsRead) {
        boolean flushOK = false;
        for (int i = 0; (i < 2) && !flushOK; i++) {
          flushOK = (CommitLog.this.mapedFileQueue.getCommittedWhere() >= req.getNextOffset());
          if (!flushOK) {
            CommitLog.this.mapedFileQueue.commit(0);
          }
        }
        req.wakeupCustomer(flushOK);
      }

      long storeTimestamp = CommitLog.this.mapedFileQueue.getStoreTimestamp();
      if (storeTimestamp > 0) {
        CommitLog.this.defaultMessageStore.getStoreCheckpoint().setPhysicMsgTimestamp(storeTimestamp);
      }
    }

  //
    public void run() {
        CommitLog.log.info(this.getServiceName() + " service started");
        while (!this.isStoped()) {
            try {
                this.waitForRunning(0);
                this.doCommit();
            } catch (Exception e) {
                CommitLog.log.warn(this.getServiceName() + " service has exception. ", e);
            }
        }
    }
```

### 4.3.2 异步刷盘流程

异步刷盘在开启transientStorePoolEnable机制会有区别。如果开启后，MQ会申请一个和commitLog文件一样大小的堆外内存，并且将该内存进行锁定确保不会被置换到虚拟内存中去。消息先被追加到堆外内存，再提交到内存映射中，最后才flush到磁盘中。步骤如下

1. 将消息追加到ByteBuffer(DirectByteBuffer)，wrotePosition往后移动
2. CommitRealTimeService线程每200ms将ByteBuffer更新的内容提交到MappedByteBuffer中。
3. FlushRealTimeService默认500ms将MappedByteBuffer中将新追加的内容写到磁盘中。
