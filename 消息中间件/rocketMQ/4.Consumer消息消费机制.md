[TOC]

# 1. 简介

## 1.1 问题

1. DefaultMQPushConsumer的本质还是pull，mq如何保证消息消费的实时性
2. 消费消息是否存在超时问题？超时后会重试么？
3. 如何表示消息消费失败和消费成功？
4. consumer端为何需要保证幂等，什么时候会重复消费消息？
5. 消息消费失败如何实现重试？

## 1.2 PUSH和PULL

Broker和Consumer之间的消息传送也有两种方式:Push和Pull。 Pull是消费端主动发起拉消息请求，而Push是消息到达消息服务器后推送给消息消费者 。在大部分场景中使用的采用的是pull方式实现。**通过pull不断轮训Broker获取消息，当不存在新消息Broker会挂起client请求，直到有消息才唤醒线程返回新消息**。该机制和broker主动push的实时性相当。

## 1.3 代码示例

这是Consumer启动的代码，需要设置消费的offset，和NameServer连接，订阅的topic等操作。

```java
DefaultMQPushConsumer consumer=new    
DefaultMQPushConsumer("testConsumer1")；
consumer.setConsumeFromWhere(ConsumeFromWhere.CONSUME_FROM_FIRST_OFFSET);
consumer.setNamesrvAddr("localhost:9876");
consumer.subscribe("TopicTest", "*");

consumer.registerMessageListener(new MessageListenerConcurrently() {
            @Override
            public ConsumeConcurrentlyStatus consumeMessage(List<MessageExt> msgs,
                ConsumeConcurrentlyContext context) {
                System.out.printf("%s Receive New Messages: %s %n", Thread.currentThread().getName(), msgs);
                return ConsumeConcurrentlyStatus.CONSUME_SUCCESS;
            }
        });
consumer.start();
System.out.printf("Consumer Started.%n");
```

# 2.数据结构

MQ消费者的数据结构主要分为2个部分，一个部分在MQClientInstance当中统一管理，无论是Consumer还是Producer；还有一部分是在Consumer和Producer区分管理，比如各自订阅的MessageQueue

## 2.1 MQClientInstance维护的数据结构

- TopicRouteData

  保存了所有的Queue信息，不管是consumer还是producer的 

  ```java
  private String orderTopicConf;//brokerName:num count
  private List<QueueData> queueDatas;
  private List<BrokerData> brokerDatas;
  private HashMap<String/* brokerAddr */, List<String>/* Filter Server */> filterServerTable;
  ```

- QueueData   通过wirte或者read来区分queue属于Consumer(read)/Producer(write)

  ```java
  private String brokerName;
  private int readQueueNums;
  private int writeQueueNums;
  private int perm;
  private int topicSynFlag;
  ```

- BrokerData    Broker的地址信息

  ```java
  private String brokerName;
  private HashMap<Long/* brokerId */, String/* broker address */> brokerAddrs;
  ```

- PullRequest   拉取的请求信息，包括所属组信息，要拉取的offset信息，Queue信息，消费进度信息

  ```java
  private String consumerGroup;
  private MessageQueue messageQueue;
  private ProcessQueue processQueue;
  private long nextOffset;
  ```

- PullMessageService   拉取信息的服务，会不断遍历每一个PullRequest进行信息的拉取

  ```java
  private final LinkedBlockingQueue<PullRequest> pullRequestQueue = new LinkedBlockingQueue<PullRequest>();
  private final MQClientInstance mQClientFactory;
  ```

## 2.2 Consumer中维护的数据结构

- TopicPublishInfo   producer保存MessageQueue的数据结构    

  ```java
  private boolean orderTopic = false;
  private boolean haveTopicRouterInfo = false;
  private List<MessageQueue> messageQueueList = new ArrayList<MessageQueue>();
  private AtomicInteger sendWhichQueue = new AtomicInteger(0);
  ```

- SubscriptionData   consumer的消费信息，包括topic，订阅的tags

  ```java
  public final static String SUB_ALL = "*";
  private boolean classFilterMode = false;
  private String topic;
  private String subString;
  private Set<String> tagsSet = new HashSet<String>();
  private Set<Integer> codeSet = new HashSet<Integer>();
  private long subVersion = System.currentTimeMillis()
  ```

- RebalanceImpl   订阅topic队列信息和订阅topic的tag信息

  ```java
  ConcurrentHashMap<String/* topic */, Set<MessageQueue>> topicSubscribeInfoTable
  ConcurrentHashMap<String /* topic */, SubscriptionData> subscriptionInner
  <MessageQueue, ProcessQueue> processQueueTable
  ```

- MessageQueue     broker的消息队列

  ```java
  private String topic;
  private String brokerName;
  private int queueId;
  ```

- ProcessQueue   消费者拉取到的消息临时队列 每个MessageQueue对应一个ProcessQueue

  ```java
  private final TreeMap<Long, MessageExt> msgTreeMap = new TreeMap<Long, MessageExt>();
  private volatile long queueOffsetMax = 0L;
  private final AtomicLong msgCount = new AtomicLong();
  ```

- RemoteBrokerOffsetStore    消费进度存储

  ```java
  private final MQClientInstance mQClientFactory;
  private final String groupName;
  private final AtomicLong storeTimesTotal = new AtomicLong(0);
  private ConcurrentHashMap<MessageQueue, AtomicLong> offsetTable =
          new ConcurrentHashMap<MessageQueue, AtomicLong>();
  ```

## 2.3 主要类介绍

- DefaultMQPushConsumer   设置组名、消费模式、消费offset、线程数量、批量拉取大小等，还有RebalanceImpl，OffsetStore，AllocateStrategy
- RebalanceService：均衡消息队列服务，负责分配当前 `Consumer` 可消费的消息队列( `MessageQueue` )。当有新的 `Consumer` 的加入或移除，都会重新分配消息队列。遍历所有的Consumer调用doBalance
- OffsetStore   消费进度 ，有2种模式  集群模式和广播模式，区别是RemoteBrokerOffsetStore和LocalFileOffsetStore。
- PullMessageService：轮训所有的PullRequest，调用pullMessage进行MessageQueue拉取消息服务，并提交消费任务到`ConsumeMessageService`
- `ConsumeMessageService`：消息具体的消费，有2种模式，`ConsumeMessageConcurrentlyService`和`ConsumeMessageOrderlyService`,用于调用MessageListener做具体的消费。
- MessageListener: 客户端实现的接口，用于业务逻辑的处理。
- `ProcessQueue` ：消息处理队列。
- `MQClientInstance` ：封装对 `Namesrv`，`Broker` 的 API调用，提供给 `Producer`、`Consumer` 使用。
- AllocateMessageQueueStrategy：分配消息的策略，将所有的MessageQueue均分到各个Instance当中。

# 3. Consumer的启动流程

consumer的启动调用的是DefaultMQPushConsumerImpl#start方法。

1. 初始化MQClientInstance，从MQClientManager中构造
2. 获取该consumer的所有topic订阅信息构建SubscriptionData并加入到RebalanceImpl当中。consumer在启动时会自动订阅重试消息的topic。topic为%RETRY%+消费组名。
3. 设置Consumer的offsetStore参数初始化OffsetStore。如果offsetStore存在就赋值，不存在就创建。如果是集群模式，就将消息进度保存在broker。如果是广播模式，就存储在消费端。
4. 初始化ConsumerMessageService。ConsumerMessageService主要负责消息的消费，内部维护了一个线程池。分为ConsumeMessageOrderlyServic和 ConsumeMessageConcurrentlyServic，区别在与MessageListener的类型。
5. 向MQClientInstance注册该consumer，并启动MQClientInstance。在一个JVM中，所有的consumer和producer持有的是同一个MQClientInstance。

consumer的一些重要相关参数

```java
> messageModel:消费消息的模式(广播模式和集群模式）
> consumeFromWhere:选择起始消费位置的方式
> allocateMessageQueueStrategy:分配具体messageQuene的策略子类。（负载均衡逻辑实现的关键类）
> consumeThreadMin：消费消息线程池的最小核心线程数(默认20)
> consumeThreadMax：最大线程数（默认64）
> pullInterval：拉取消息的间隔，默认是0
> consumeMessageBatchMaxSize：每批次消费消息的条数，默认为1
> pullBatchSize：每批次拉取消息的条数，默认32
```

# 4. Consumer的消息拉取

消息拉取的整体流程图如下所示

![](https://medesqure.oss-cn-hangzhou.aliyuncs.com/img/20200801181956.png)

消息的消费有2种模式，广播模式和集群模式。

- 广播模式   集群的所有消费者都要拉取订阅主题下的所有MessageQueue的消息。offset存在consumer当中，比较简单。
- 集群模式  一个consumerGroup中有多个consuemr，每个topic存在多个MessageQueue。需要考虑负载均衡，offsetStore存在broker当中。

消息拉取的流程图

![](https://medesqure.oss-cn-hangzhou.aliyuncs.com/img/20200731003502.png)

## 4.1  PullRequest消息拉取任务

从一个MessageQueue中拉取一次消息任务对应的是PullRequest。

```java
public class PullRequest {
    //消费组
    private String consumerGroup;
    //要消费的队列
    private MessageQueue messageQueue;
    //消息处理队列，从broker拉取到的消息先存入到processQueue，然后再提交到消费线程中进行消费
    private ProcessQueue processQueue;
    //待拉取的consumequeue的offset
    private long nextOffset;
    private boolean lockedFirst = false;
}
```

ProcessQueue还订阅了Retry的topic，该topic是以Consumer为维度的，不是topic维度。

![](https://medesqure.oss-cn-hangzhou.aliyuncs.com/img/20200802182415.png)

## 4.2  PullRequest的创建时机

存放的pullRequest封装的是每一个consumergroup以及负责消费的消费队列messageQuene。

PullRequest的创建有2个地方

- **MQPushConsumer#PullMessage的PullCallBack**

  PullCallback是PullMessageService的回调类，在完成一次的PullRequest的消息拉取任务后都会设置该PullRequest的下一次offsetonSuccess和onException中都会调用了pullRequestQueue的put。
  而PullCallback实际就是每次拉取消息之后的回调类。也就是保证了拉完消息之后都会调用pullRequestQueue的put逻辑

- RebalanceService.run负载均衡

  RebalanceService是针对consumer做messageQueue的负载均衡策略。当consumer挂了就要重新做rebalance。将messageQueue按照存活的consumer节点做分配。

  1. 在重新需要分配messageQueue时，将新创建的pullRequest都放在pullRequestList。
  2. 将pullRequestList再放到pullRequestQueue。并且会将老的pullRequest移除。

所以，可以得到如下的结果

在consumer启动的时候，RebalanceService会创建PullRequest使的pullRequestQueue有值。PullMessageService的线程会从Queue中不断获取消息，在处理完后重置PullRequest再次放入Queue中。

## 4.3 PullMessageService 拉取消息

MQ使用一个单独的线程PullMessageService负责消息的拉取，该类继承了ServiceThread。该线程会从pullRequestQueue中获取一个pullRequest消息拉取任务。

**PullMessageService提供了方法将pullRequest添加到pullRequestQueue当中。加入有2种方式: 立即add和延迟add(使用ScheduleExecutorService)**

> **PullMessageService拉取消息是一个并发拉取的机制。**在pullMessage这个过程是一个provider-consumer的机制。该线程一直从Queue当中获取任务并执行。具体拉取任务是提交到Netty线程池中执行。在拉取任务的线程执行完毕后会再将任务投递到PullRequestQueue。

![](https://medesqure.oss-cn-hangzhou.aliyuncs.com/img/20200802215755.png)

```java
//ServiceThread继承了Runnable接口
public class PullMessageService extends ServiceThread {
        @Override
    public void run() {
        while (!this.isStopped()) {
            //从阻塞队列pullRequestQueue中拿pullRequest。
            PullRequest pullRequest = this.pullRequestQueue.take();
            this.pullMessage(pullRequest);
        }
    }
}
```

## 4.4 this.pullMessage(pullRequest)消息拉取的步骤

在PullMessageService#PullMessage中是具体的从broker中拉取消息的流程。具体的步骤如下

1. 从PullRequest中获取ProcessQueue进行消息拉取的流控。从消息的数量和消费间隔2个维度进行控制。

2. 拉取该主题订阅消息，如果为空就结束本次消息的拉取，该队列的下一次拉取任务PullRequest延迟3s再放入到PullRequestQueue当中。

3. 从PullRequest中获取brokerName和BrokerId，并从MQClientInstance中获取broker的具体地址。broker是主从机制。

4. 构建消息拉取的网络请求对象RemoteCommand。**这里需要注意的是拉取模式有同步拉取和异步拉取**。

   异步拉取会采用pullCallback进行回调。具体和broker中拉取的消息的任务是交到netty的线程去执行。

![](https://medesqure.oss-cn-hangzhou.aliyuncs.com/img/20200802220915.png)

5. 执行PullCallBack的onSuccess逻辑  这里会计算下次拉取的offset并更新pullRequest，提交任务。

   ![](https://medesqure.oss-cn-hangzhou.aliyuncs.com/img/20200802231127.png)

6. 将拉取到的消息交给consumeMessageService,其实就是将任务提交到线程池中去消费。

   ```java
   ConsumeRequest consumeRequest = new ConsumeRequest(msgs, processQueue, messageQueue);
   this.consumeExecutor.submit(consumeRequest);
   ```

一个consumer客户端会分配一个拉取消息线程（PullMessageService），不停地从存放了messageQuene的阻塞队列中take需要拉取消息的messagequene，最后通过调用通知网络层发起拉取消息拉取的网络请求（实际就是交给netty的worker线程拉消息），netty的worker线程拉取到消息后调用处理PullCallback处理拉取的结果。

**由于从broker拉取消息的网络请求交给了netty的worker线程处理，并且work线程处理完之后再异步通知拉取结果处理，我们可以知道pullmessage本身并没有太重的操作，同时每次请求broker拉取消息是批量拉取（默认值是每批32条），因此即使一个consuemr客户端只会有一个线程负责所有consumerGroup，也不会有太慢以及太大的性能瓶颈。**

## 4.5 Broker和Consumer之间的交互

broker端处理消息拉取的入口: PullMessageProcessor#processRequest。步骤如下

1. 调用MessageStore.getMessage查找消息，参数有group，topic，queuId，offset，maxMsgNums，Filter。
2. 根据topic和queueId获取ConsumeQueue，根据传入的offset和ConsumeQueue的最大最小offset偏移判断当前拉取是否有效。有效就拉取对应的消息
3. 根据主从同步延迟，如果S节点包含下一次拉取的偏移量，下一次拉取任务的brokerId设置为S。
4. 如果当前节点是master节点，就更新消息消费进度。

## 4.6 ConsumeMessageService的消息消费过程

ConsumeMessageService提供了2种方式的消息消费，ConsumMessageOrderlyService和ConsumeMessageConcurrentlyService。

消息具体消费的过程是将ConsumeRequest提交到具体的线程池中去执行。ConsumeMessageConcurrentlyService会创建一个线程池去消费消息。

![](https://medesqure.oss-cn-hangzhou.aliyuncs.com/img/20200802232139.png)

其中，线程池的大小由用户自己来控制。

**消息消费具体是取得业务方实现的messageListener，调用该逻辑得到处理的结果。**

在源码中可以得到如下的信息

1. 调用业务实现的消费消息逻辑，得到消费消息结果（即使消费超时了，也最终会根据messageListener执行返回的结果来决定是否重新消费消息)
2. 根据消费模式不同对消费失败的结果做不同处理。**对于广播模式，失败了消息会直接丢弃；集群模式会重新消费消息。**
3. **需要重新消费的消息会将消息重新发送至broker。**
4. 不管消息消费是否成功，都会更新consumerGroup消费到的offset。

## 4.7 Offset的更新

消费完消息后会将MessageQueue对应的offset存放到map中进行缓存。

```java
@Override
public void updateOffset(MessageQueue mq, long offset, boolean increaseOnly) {
    if (mq != null) {
        //实际每个messageQuene消费到的offset存放在offsetTable缓存map中。
        AtomicLong offsetOld = this.offsetTable.get(mq);
        if (null == offsetOld) {
            offsetOld = this.offsetTable.putIfAbsent(mq, new AtomicLong(offset));
        }
        if (null != offsetOld) {
            if (increaseOnly) {
                MixAll.compareAndIncreaseOnly(offsetOld, offset);
            } else {
                offsetOld.set(offset);
            }
        }
    }
}
```

offset的持久化到broker应该是定时任务提交到broker当中。Consumer的启动会创建一个定时任务。具体的提交由OffsetStore的实现类，将缓存的offsetTable提交到broker当中。

![](https://medesqure.oss-cn-hangzhou.aliyuncs.com/img/20200803015755.png)

所以，offset是先在内存OffsetStore中保存，然后再异步持久化到broker当中。

### 4.7.1 Offset的设计思考

**集群模式下以主题与消费组为键保存该主题所有队列的消费进度** 。

消费者线程池每处理完一个消息消费任务ConsumeRequest时会从 ProceeQueue 中移除本批消费的消息，并返回ProcessQueue中最小的偏移量。用该偏移量更新消息 队列消费进度，也就是说更新消费进度与消费任务中的消息没什么关系 。

- 消息进度的推进(都是以processQueue中的最小偏移量为准)

例如现在两个消费

任务 task1 ( queueOffset 分别为 20,40 )， task2 ( 50,70 ），并且 ProceeQueue 中当前包含最小消息偏移量为 10 的消息 ， 则 task2消费结束后，将使用10去更新消费进度， 并不会是70 。 当 taskl 消费结束后 ，还是 以 10 去更新消费队列消息进度，消息消费进度的推进取决于 ProceeQueue 中偏移量最小的消息消费速度 。 如果偏移量为10 的消息消费成功后，假如ProceeQueue中包含消息偏移量为 100 的 消息， 则消 息偏移量为 10 的消息消 费成功后，将直接用100更新消息消费进度 。

- 消费死锁导致的消费进度无法推进

 **那如果在消费消息偏移量为10 的消息时发送了死锁导致一直无法被消费， 那岂不是消息进度无法向前推进 。 为 了避免这种情况， RocketMQ引入了 一 种消息拉取流 控 措施：** DefaultMQPushConsumer#consumeConcurrentlyMaxSpan=2000 ，消息处理队列 ProceeQueue 中最大消息偏移与最小偏移量不能超过该值，如超过该值，触发流控，将延迟该消息队列的消息拉取 。 

- rebalance导致的offset更新

触发消息消费进度更新的另外一个是在进行消息负载时，如果消息消费队列被分配给其他消费者时，此时会将该 ProceeQueue状态设置 drop时，持久化该消息队列的消费进度，并从内存中移除 。 

所以，我们可以得到如下的结果

1. 由于是先消费消息，再提交offset，因此可能存在消费完消息之后，提交offset失败；当然这种可能性极低（因为消费完之后提交offset只是做了内存操作）
2. 由于offset是先存在内存中，定时器间隔几秒提交给broker，消费之后的offset是完全存在可能丢失的风险（例如consumer端突然宕机），从而会导致没有提交offset到broker，再次启动consumer客户端时，会重复消费。

因此，consumer的业务消费代码一定要保证消息的幂等。

**如果offset都是依赖定时器提交broker，那应用正常关闭的时候，是不是offset丢失的概率很大？其实不然，我们观察consumer的shutdown方法会主动触发一次持久化offset到broker的方法。**

![](https://medesqure.oss-cn-hangzhou.aliyuncs.com/img/20200803014759.png)

## 4.8 消息的重试原理

consumer将消费失败的消息会重新发送给broker。同时，如果重新发送给broker也依然失败了，则会将原来失败的消息交给定时任务重新尝试消费。

consumer端将失败消息重新发送给broker会使用RequestCode.CONSUMER_SEND_MSG_BACK类型的RemotingCommand。

根据处理失败消息过程可以得到两个非常关键的信息：

> 1、失败的消息会将topic转换为%RETRY%+groupName的topic名称；

> 2、此失败的消息会设置延迟级别（转化成了延迟消息）；

因为消息失败了立即重试可能还是会失败，所以要转化成延迟的消息。

**消息消费失败，消费端conusmer会将该消息重新再发给broker，broker接收到该消息，会作为延迟消息存放起来（因为重试消息是有时间间隔），利用延迟消息的功能，broker端到了延迟的时间点，再将该延迟消息转换为重试消息（%RETRY%+consumerGroup），此时consumer端可见，从而会拉取到该重试消息，从而达到延迟重复消费的目的。**

## 5. Consumer的流量控制

pullMessageService是一直从queue中拉取任务并提交任务。所以消息一直会被拉取到consumer本地缓存在processQueue当中。如果不做流量的控制，那么本地的内存会被打爆。

consumer可以增大ConsumerMessageService的线程池大小来控制消费的速度。MQ也可以通过ProcessQueue来控制消息拉取的速度。

## 5.1 为何需要ProcessQueue

consumer通过PullMessageService从MessageQueue当中pull消息。**如果pull消息后直接提交到线程池中将很难进行监控和流量控制。例如消息堆积数量、重复处理消息、延迟处理消息等**。rocketmq通过一个快照类ProcessQueue，在pushConsumer运行的时候，每个Message Queue都会对应一个ProcessQueue对象，保存该Message Queue消息处理状态的快照。

ProcessQueue通过TreeMap和读写锁实现的。TreeMap以offset作为key，消息内容为value。保存了所有从MessageQueue中获取到但是没有被处理的消息。

## 5.2 ProcessQueue如何控制消息拉取的速度

ProcessQueue可以从3个维度做消费端的流量控制。

1. 消息处理的总数   当ProcessQueue数组的消息总数超过1000，就会放弃本次本次拉取动作，并且该ProcessQueue的下次拉取任务延时50ms。
2. 消息的总量限制   当ProcessQueue的消息大小超过100M，也会触发流控。
3. 消息的偏移量限制  当队列中的最大偏移和最小偏移超过2000，就会触发流控。**防止一条消息堵塞，导致消费进度无法向前推进，造成大量的重复消费**。比如有一个消息一直无法被消费，其后的消息被消费就会造成偏移量超过2000。

流量控制相关代码如下

```java
public void pullMessage(final PullRequest pullRequest) {
       ...
           // 如果processQueue消息数量>队列级别的流量控制阈值，默认情况下，每个消息队列最多缓存1000条消息
         if (cachedMessageCount > this.defaultMQPushConsumer.getPullThresholdForQueue()) {
            this.executePullRequestLater(pullRequest, PULL_TIME_DELAY_MILLS_WHEN_FLOW_CONTROL);
            return;
        }
        // 如果队列缓存消息的大小超过100M（默认），考虑到批量消息瞬时可能超过100M
        if (cachedMessageSizeInMiB > this.defaultMQPushConsumer.getPullThresholdSizeForQueue()) {
            this.executePullRequestLater(pullRequest, PULL_TIME_DELAY_MILLS_WHEN_FLOW_CONTROL);
            return;
        }
        if (!this.consumeOrderly) {
            // 判断是否大于最大消息便宜跨度（默认2000）
            if (processQueue.getMaxSpan() > this.defaultMQPushConsumer.getConsumeConcurrentlyMaxSpan()) {
                this.executePullRequestLater(pullRequest, PULL_TIME_DELAY_MILLS_WHEN_FLOW_CONTROL);
                return;
            }
        }
       ...
 }
```

# 6. Rebalance负载均衡

集群内有多个Consumer，如何负载Topic下的多个MessageQueue，如果有新的Consumer加入或者离开，消息队列如何重新分布。

MQ的MessageQueue是由rebalanceService线程来实现。一个MQClientInstance会持有一个RebalanceService的实现，并在MQClientInstance启动而启动。

RebalanceService线程默认20s执行一次MessageQueue的rebalance。具体是MQClientInstance会遍历所有注册的Consumer，对消费者执行doRebalance()方法。

每个DefaultMQPushConsumer会持有一个RebalanceImpl对象，该对象会遍历consumer订阅的topic的所有messageQueue进行重新负载。即Map<String,SubscriptionData> subTable。consumer在启动的时候会上报订阅的topic信息。

## 6.1 RebalanceImpl#rebalanceByTopic队列重新负载均衡

1. 从TopicPublishInfo缓存中获取topic的所有messageQueue队列信息。
2. 发送请求从broker中获取该consumeGroup的所有consumer的clientID。topic的队列可能会分布在多个broker当中，每个broker中都存有全量的MessageQueue信息。
3. **对ClientIdSet和MessageQueue排序，保证同一个消息队列不会被多个消费者分配。**在一次rebalance计算后得到consumer分配到的新的消息队列负载List<MessageQueue> mqSet。
   MQ提供的队列分配算法有5种，如果有q1-q8 的8个消息队列，c1-c3的3个消费者。那么分配算法如下
   1. 平均分配   c1:q1~q3 c2:q4~q6  c3:q7~q8
   2. 平均轮训分配   c1: q1,4,7；c2: q2,5,8 c3:q3,6
   3. 一致性哈希    不推荐使用，负载信息不容易跟踪
4. consumer负载的消息队列缓存表为ConcurrentMap<MessageQueue， ProcessQueue> processQueueTable。如果缓存中的MessageQueue不在新的mqSet中，说明该MessageQueue被分配到其他的consuemr，那么就把对应的ProcessQueue暂停， 即状态drop=true，里面的消息就不会再被消费。并将该consumer消费该消息的offset进度移除。
5. 遍历mqSet，如果不在processQueueTable当中，就说明是新增加的messageQueue。先从OffsetStore中读取消费的进度，如果读取不到消费进度，MQ提供了3种模式获取消费进度。从consumer启动时间开始读取，从队列最新偏移量开始读取等等
6. 将PullRequest加入到PullRequestQueue当中，开始消息的拉取。

## 6.2 总结

- PullRequest 对 象在 什 么 时候创 建并 加入到 pullR巳questQueue 中以便唤醒PullMessageService 线程 。 RebalanceService线程每隔 2 0s 对 消 费者订 阅 的主题进行一次 队列重新分配 ， 每一次 分配都会获取主题的所有队列、从 Broker 服务器实时查询当前该主题该消费组内消费者列表 ， 对新分配的消息队列会创建对应的 PullRequest 对象 。 在一个 JVM 进程中，同一个消费组同一个队列只会存在一个 PullRequest 对象 。 

- 集群内多个消费者是如何负载主题下的多个消费队列 ，并且如果有新的消费者加入时，消息队列又会如何重新分布 。

  由于每次进行队列重新负载时会从 Broker 实时查询出当前消费组内所有消费者，并且对消息队列、消费者列表进行排序，这样新加入的消费者就会在队列重新分布时分配到消费队列从而消费消息 。
  本节分析了消息队列的负载机制




