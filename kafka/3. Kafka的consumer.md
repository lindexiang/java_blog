# 1. High Level Consumer

consumer不需要关心offset，zk里面存储了topic的所有partition的offset。 KAKFA的High Level Consumer提供了一个屏蔽底层细节的抽象。

High Level Consumer 将每个group消息的某个partition的最后一条消息的offset存在zk当中。这个offset是根据consumer group名字来确定的。

**每个High Level Consumer都属于一个 consumer group，不指定就是默认的consumer。**

![](https://medesqure.oss-cn-hangzhou.aliyuncs.com/img/20200716211119.png)



> kafka支持离线处理和实时处理。只要创建一个consumer group去实时消费消息即可。

每个consumer实例都只创建一个MessageStream。所有一般情况下，一个服务器实例只需要创建一个实例。**一台机器一个consumer去拉2个partition和2个consumer去拉2个partition其实性能差不多。**

## 2. High Level Consumer Rebalance

kafka保证一个consumer group只有一个consumer消费消息。每个consumer 实例消费一个或者多个partition。

优点:

consumer不用和很多的broker通信，减少了通信的开销。保证了partition内消费有序，topic的消息是无序。

> 当consumer的数量大于partition的数量，那么有consumer会消费不到消息。所以注意conusmer的数量。

## 2.1 Consumer rebalance 的步骤

在老版本的consumer 的rebalance是没有中心协调器。根据一定的算法直接计算出consumer要消费那些partition。计算方式如下

1. 得到目标topic下的所有的partition，并排序
2. 将consumer group下的所有consumer排序
3. i * N 到 (i+1) * N - 1个partition分配给Ci。其中i是consumer的下标，N是partition对consumer取余

**从这个计算方式得出consumer每次要拉取的partition是固定的，**

当consumer注册触发Consumer Group的rebalance如下

1. High Level Consumer 启动将自身的Id注册到Consumer group下，在zk上的路径为/consumers/consumer group/ids/consumer id
2. 在consumers/consumer group/ids上注册watch
3. 在brokers/ids上注册watch

**在这种策略下，每一个consumer 或者broker的增加减少都会触发consumer balance。以为从上面的技工方式可以得出每个consumer只会调整自己的partition。所以每个consumer都需要调整。**

该方式的问题:

1. 任何的broker或者consumer的增减都会所有consumer的rebalance
2. 每个conusmer是通过监听zk来判断broker或者consumer宕机，但是存在一致性的问题导致rebalance会失败，
3. consumer无法知道其他的consumer是否rebalance成功，导致kafka处在不正确的状态。

## 2.2 consumer rebalance的Coordinator

针对以上的问题，可以采用 broker controller类似的方案。即在consumer group中选出一个coordinator(中心协调器).由它来watch zk来判断是否有partition或者consumer的增减，然后进行rebance并发送RPC命令到相关的Consumer执行。如果不成功就重试。

步骤如下

1. Consumer 启动，向Broker列表中的任一Broker发送MetaDataRequest请求，获取Group的Coordinator信息。
2. consumer连接到Coordinator并建立心跳。如果心跳返回没有错误码 consumer开始向partition 拉取数据。
3. 如果心跳返回rebalance，consumer停止pull数据，并提交offset。从coordinator中拉取新的partition列表进行rebalance。
4. consumer继续pull数据。

![](https://medesqure.oss-cn-hangzhou.aliyuncs.com/img/20200716220230.png)

# 3. Low Level Consumer

使用该consumer是用户希望自己去控制消费的offset。可以做如下的事情

- 同一条消息读多次
- 只读取某个Topic的部分Partition
- 管理事务，从而确保每条消息被处理一次，且仅被处理一次

与Consumer Group相比，Low Level Consumer要求用户做大量的额外工作。

- 必须在应用程序中跟踪offset，从而确定下一条应该消费哪条消息
- 应用程序需要通过程序获知每个Partition的Leader是谁
- 必须处理Leader的变化

　　使用Low Level Consumer的一般流程如下

- 查找到一个“活着”的Broker，并且找出每个Partition的Leader
- 找出每个Partition的Follower
- 定义好请求，该请求应该能描述应用程序需要哪些数据
- Fetch数据
- 识别Leader的变化，并对之作出必要的响应



# 4. consumer的故障检测机制

consumer加入到group后会和coordinator同时开始故障检测程序。

- consumer向coor发送心跳

**Consumer周期向coor发送heartbeat并等待响应。如果没有收到就认为coor挂了，向broker发送消息拉取新的配置列表。**

- coor向consumer发送心跳

如果coor在周期内没有收到consumer的heartbeat，就认为该consumer挂了，发起rebalance请求。

在rebalance的过程中，consumer可能会重新heartbeat。那么这个时候会被拒绝请求直到rebalance结束。然后再触发一次consumer加入的过程。



# 5. Coordinator的故障转移

在初始化的时候，consumer group中的所有节点都会先去zk注册一个coordinator直到成功，其他watch。

coordinator开始管理这个group内的所有节点。

一个正常的rebalance的阶段如下

1. topic/partition的改变或者consumer的改变都要触发coordinator在zk上的watch并执行rebalance
2. coordinator通过heartbeat向consumer返回错误码并发起rebalance
3. Consumer 发起request获取新的partition列表
4. **coordinator在zk中增加group的generation Id并将partition的分配情况写入zk**
5. coordinator发送response 将partition列表返回客户端。

